{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joel224/Marketplace-Agents/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT39FL0J_KQL"
      },
      "source": [
        "Local RAG with In-Memory Caching This plan outlines the design for two main agents—a Price Suggestor and a Chat Moderation agent—that will work with your dataset in a text file and use a local Gemini API on Colab.\n",
        "\n",
        "Data Ingestion and Local Knowledge Base Since a vector database is not an option, the dataset will be loaded directly into memory to create a searchable knowledge base.\n",
        "Objective: Read the text file at /content/drive/MyDrive/db.txt and convert it into a structured, in-memory format.\n",
        "\n",
        "Process:\n",
        "\n",
        "Load Data: Read the entire text file line by line. the data is not csv currently & temporary its txt , path :/content/drive/MyDrive/db.txt\n",
        "\n",
        "dataset look like this :\n",
        "\n",
        "id,title,category,brand,condition,age_months,asking_price,location 1,iPhone 12,Mobile,Apple,Good,24,35000,Mumbai 2,Redmi Note 11,Mobile,Xiaomi,Like New,8,11000,Delhi 3,OnePlus Nord 2,Mobile,OnePlus,Fair,30,15000,Bangalore......\n",
        "\n",
        "Parse Records: For each line, parse the comma-separated values to extract individual product details: id, title, category, brand, condition, age_months, asking_price, and location.\n",
        "\n",
        "In-Memory Storage: Store these parsed product records in a suitable data structure, such as a list of dictionaries or a pandas DataFrame. This will serve as your local, searchable knowledge base.\n",
        "\n",
        "Indexing: Create in-memory indices (e.g., hash maps or dictionaries) for fast lookups based on key attributes like category, brand, and condition. This mimics a simple database index and is crucial for efficient \"retrieval.\"\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) with a Local LLM The core of your system will be the RAG framework, where you \"retrieve\" information from your in-memory knowledge base and then use it to \"augment\" the prompt for the local Gemini LLM.\n",
        "Objective: Combine a user's query with relevant data from your in-memory knowledge base to get a more accurate LLM response.\n",
        "\n",
        "Process:\n",
        "\n",
        "User Query: A user submits a query (e.g., \"What is a fair price for a used iPhone 12 in good condition?\").\n",
        "\n",
        "Information Extraction: The agent extracts key information from the user's query, such as the category, brand, and condition.\n",
        "\n",
        "Local Retrieval: The agent uses the extracted information to search your in-memory knowledge base. Instead of a vector similarity search, it will perform a keyword-based search or filter the records based on matching category, brand, and condition values.\n",
        "\n",
        "Context Augmentation: The agent selects the most relevant product records from the search results. These records are formatted into a concise context string (e.g., \"Relevant products found in the database: [product data 1], [product data 2], ...\").\n",
        "\n",
        "LLM Prompting: The agent constructs a final prompt by combining the user's original query, a system instruction, and the retrieved context string. This is then sent to the local Gemini API.\n",
        "\n",
        "Response Generation: The LLM generates a response based on this augmented prompt, providing a fair price range and reasoning grounded in the provided data.\n",
        "\n",
        "Adaptive Replacement Cache (ARC) Implementation Adaptive Replacement Cache (ARC) will be implemented as a layer on top of your in-memory knowledge base to optimize retrieval times for frequently accessed items.\n",
        "Objective: Improve performance by caching recent and frequent queries and their corresponding retrieval results.\n",
        "\n",
        "Process:\n",
        "\n",
        "Cache Structure: Implement ARC using two lists or dictionaries: one for recently accessed items and one for frequently accessed items. You can store the search queries and their retrieved product data.\n",
        "\n",
        "Caching Logic: When a user's query comes in, the agent will first check the ARC cache.\n",
        "\n",
        "Cache Hit: If the query is found, the cached result is used immediately, bypassing the search on the full knowledge base.\n",
        "\n",
        "Cache Miss: If the query is not in the cache, the agent performs the local retrieval (as described above), retrieves the relevant data, and then stores this new query and its results in the ARC cache, following the ARC algorithm's rules to manage what is evicted.\n",
        "\n",
        "Agent Specific Logic Price Suggestor Agent:\n",
        "Input: Product details (e.g., category, age_months, condition).\n",
        "\n",
        "Logic: Uses the RAG process described above. The retrieval step will find similar items in the in-memory knowledge base, and the LLM will use this information to calculate a fair price range and a reasoning, as specified in your project document.\n",
        "\n",
        "Chat Moderation Agent:\n",
        "\n",
        "Input: A chat message.\n",
        "\n",
        "Logic: This agent does not need to query the product dataset directly. It can be a simple LLM call with a \"system\" prompt that contains your moderation policies and rules. The prompt would instruct the LLM to classify the message as \"Safe,\" \"Abusive,\" \"Spam,\" or \"Contains Mobile Number\" and provide a reason, based on its training and the rules provided in the prompt. This keeps the task simple and efficient.\n",
        "\n",
        "Technical Stack and Deliverables Local LLM: Use the Gemini API hosted locally on Google Colab.\n",
        "Local Caching: Implement ARC from scratch or use a local in-memory caching library that can be adapted.\n",
        "\n",
        "Framework: You can use a Python framework like Flask or FastAPI for the API endpoints, as mentioned in your project document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706aaa82"
      },
      "source": [
        "Next, I will modify the `rag_process` function in the existing cell to include the call to the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "985f80d6",
        "outputId": "33a3590e-05b0-4bde-ceb7-a29198011c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ollama setup...\n",
            "Pulling model: mistral...\n",
            "Model mistral pulled successfully.\n",
            "Dataset loaded successfully.\n",
            "\n",
            "* Flask app is running and accessible at: NgrokTunnel: \"https://f0da114547d8.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "\n",
            "* Open this URL in your browser to use the unified chat interface.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 05:59:30] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 05:59:31] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: Price_Suggestion. Entities: {'category': 'mobile', 'price': 50000}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 05:59:55] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: Price_Suggestion. Entities: {'category': 'phone', 'price': 9876543210}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:00:17] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: Price_Suggestion. Entities: {'category': 'uncertain', 'price': 30000}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:01:04] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:02:15] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: General_Question or parsing failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:02:25] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: General_Question or parsing failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:02:35] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: General_Question or parsing failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:02:47] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: General_Question or parsing failed.\n",
            "Intent: Price_Suggestion. Entities: {'category': 'unknown', 'price': 7973274}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:03:00] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: Price_Suggestion. Entities: {'category': 'phone', 'price': 23408453}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:03:57] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:04:54] \"GET / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: Price_Suggestion. Entities: {'category': 'mobile', 'price': 30000}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:05:07] \"POST /chat HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:05:16] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache hit!\n",
            "Intent: Price_Suggestion. Entities: {'category': 'mobile', 'price': 9876543210}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Sep/2025 06:06:11] \"POST /chat HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ollama setup...\n",
            "Pulling model: mistral...\n",
            "Model mistral pulled successfully.\n",
            "Dataset loaded successfully.\n",
            "\n",
            "* Flask app is running and accessible at: NgrokTunnel: \"https://23beb9270a3e.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "\n",
            "* Open this URL in your browser to use the unified chat interface.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install Flask pyngrok --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from flask import Flask, request, jsonify, session\n",
        "\n",
        "# --- Ollama Setup ---\n",
        "def start_ollama_server(model_name=\"mistral\"):\n",
        "    try:\n",
        "        print(\"Starting Ollama setup...\")\n",
        "        subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "        ollama_thread = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"], check=True))\n",
        "        ollama_thread.start()\n",
        "        time.sleep(5)\n",
        "        print(f\"Pulling model: {model_name}...\")\n",
        "        subprocess.run([\"ollama\", \"pull\", model_name], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "        print(f\"Model {model_name} pulled successfully.\")\n",
        "        return model_name\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Ollama setup: {e}\")\n",
        "        return None\n",
        "\n",
        "# Start the Ollama server and pull the model\n",
        "OLLAMA_MODEL = start_ollama_server()\n",
        "\n",
        "\n",
        "# --- Data Ingestion and Local Knowledge Base ---\n",
        "file_path = '/content/drive/MyDrive/db.txt'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    category_index = {key: list(value) for key, value in df.groupby('category').groups.items()}\n",
        "    brand_index = {key: list(value) for key, value in df.groupby('brand').groups.items()}\n",
        "    condition_index = {key: list(value) for key, value in df.groupby('condition').groups.items()}\n",
        "\n",
        "# Mapping for common user terms to database categories\n",
        "CATEGORY_MAPPING = {\n",
        "    'phone': 'Mobile',\n",
        "    'phones': 'Mobile',\n",
        "    'mobile': 'Mobile',\n",
        "    'mobiles': 'Mobile',\n",
        "    'camera': 'Camera',\n",
        "    'lenses': 'Camera Lens',\n",
        "    'lens': 'Camera Lens',\n",
        "    'tablet': 'Tablet',\n",
        "    'ipads': 'Tablet',\n",
        "    'tv': 'TV',\n",
        "    'televisions': 'TV',\n",
        "}\n",
        "\n",
        "# --- Adaptive Replacement Cache (ARC) Implementation ---\n",
        "class ARCCache:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.t1 = OrderedDict()\n",
        "        self.t2 = OrderedDict()\n",
        "        self.b1 = OrderedDict()\n",
        "        self.b2 = OrderedDict()\n",
        "        self.p = 0\n",
        "\n",
        "    def get(self, key):\n",
        "        if key in self.t1:\n",
        "            value = self.t1.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "            return value\n",
        "        elif key in self.t2:\n",
        "            value = self.t2.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "            return value\n",
        "        return None\n",
        "\n",
        "    def put(self, key, value):\n",
        "        if key in self.t1:\n",
        "            self.t1[key] = value\n",
        "            return\n",
        "        if key in self.t2:\n",
        "            self.t2[key] = value\n",
        "            return\n",
        "\n",
        "        if len(self.t1) + len(self.t2) == self.capacity:\n",
        "            if len(self.t1) > self.p:\n",
        "                old_key, _ = self.t1.popitem(last=False)\n",
        "                self.b1[old_key] = None\n",
        "            else:\n",
        "                old_key, _ = self.t2.popitem(last=False)\n",
        "                self.b2[old_key] = None\n",
        "\n",
        "        if key in self.b1:\n",
        "            self.p = min(self.capacity, self.p + max(1, len(self.b2) / (len(self.b1) + 1)))\n",
        "            self.b1.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "        elif key in self.b2:\n",
        "            self.p = max(0, self.p - max(1, len(self.b1) / (len(self.b2) + 1)))\n",
        "            self.b2.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "        else:\n",
        "            self.t1[key] = value\n",
        "            self.t1.move_to_end(key)\n",
        "\n",
        "        if len(self.b1) > self.capacity - self.p:\n",
        "            self.b1.popitem(last=False)\n",
        "        if len(self.b2) > self.p:\n",
        "            self.b2.popitem(last=False)\n",
        "\n",
        "cache_capacity = 100\n",
        "arc_cache = ARCCache(cache_capacity)\n",
        "\n",
        "\n",
        "# --- Core LLM Communication Function ---\n",
        "def call_ollama(messages, format=\"json\"):\n",
        "    if OLLAMA_MODEL is None:\n",
        "        return {\"status\": \"error\", \"reason\": \"Ollama server not running or model not available.\"}\n",
        "\n",
        "    try:\n",
        "        url = \"http://localhost:11434/api/chat\"\n",
        "        payload = {\n",
        "            \"model\": OLLAMA_MODEL,\n",
        "            \"messages\": messages,\n",
        "            \"stream\": False,\n",
        "            \"format\": format\n",
        "        }\n",
        "        response = requests.post(url, json=payload, timeout=300)\n",
        "        response.raise_for_status()\n",
        "        response_json = response.json()\n",
        "        return response_json['message']['content']\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"Error calling Ollama API: {e}. Attempting to parse raw response.\")\n",
        "        return json.dumps({\"status\": \"error\", \"reason\": f\"Ollama API returned an error: {e}\"})\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error calling Ollama API: {e}\")\n",
        "        return json.dumps({\"status\": \"error\", \"reason\": f\"Error connecting to Ollama API: {e}\"})\n",
        "\n",
        "\n",
        "# --- Retrieval Function with Price Filter ---\n",
        "def retrieve_data(query_info):\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "    filtered_df = df.copy()\n",
        "\n",
        "    # Use mapped category if it exists\n",
        "    category = query_info.get('category')\n",
        "    if category:\n",
        "        mapped_category = CATEGORY_MAPPING.get(category.lower(), category)\n",
        "        filtered_df = filtered_df[filtered_df['category'].str.contains(mapped_category, case=False, na=False)]\n",
        "\n",
        "    if 'brand' in query_info and query_info['brand']:\n",
        "        filtered_df = filtered_df[filtered_df['brand'].str.contains(query_info['brand'], case=False, na=False)]\n",
        "    if 'condition' in query_info and query_info['condition']:\n",
        "        filtered_df = filtered_df[filtered_df['condition'].str.contains(query_info['condition'], case=False, na=False)]\n",
        "\n",
        "    if 'price' in query_info and query_info['price']:\n",
        "        try:\n",
        "            price_limit = int(query_info['price'])\n",
        "            filtered_df = filtered_df[pd.to_numeric(filtered_df['asking_price'], errors='coerce') <= price_limit]\n",
        "        except (ValueError, KeyError):\n",
        "            pass\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def format_context(retrieved_data):\n",
        "    if retrieved_data.empty:\n",
        "        return \"No relevant products found in the database.\"\n",
        "    context = \"Relevant products found in the database:\\n\"\n",
        "    for _, row in retrieved_data.iterrows():\n",
        "        context += f\"- ID: {row['id']}, Title: {row['title']}, Category: {row['category']}, Brand: {row['brand']}, Condition: {row['condition']}, Age: {row['age_months']} months, Asking Price: {row['asking_price']}, Location: {row['location']}\\n\"\n",
        "    return context\n",
        "\n",
        "\n",
        "# --- Unified Chat Router Agent ---\n",
        "def router_agent(user_message):\n",
        "    cached_result = arc_cache.get(user_message)\n",
        "    if cached_result:\n",
        "        print(\"Cache hit!\")\n",
        "        return cached_result\n",
        "\n",
        "    greetings = [\"hi\", \"hello\", \"hey\", \"hallo\"]\n",
        "    if user_message.strip().lower() in greetings:\n",
        "        return {\n",
        "            \"prompt\": [],\n",
        "            \"response\": \"Hello! I can help you with product price suggestions or moderate chat messages. What can I do for you?\",\n",
        "            \"type\": \"Greeting\"\n",
        "        }\n",
        "\n",
        "    intent_system_instruction = (\n",
        "        \"You are an intent classifier and entity extractor. Analyze the user's message to determine the primary intent and extract key entities. \"\n",
        "        \"Intents are: 'Price_Suggestion', 'Moderation', or 'General_Question'. \"\n",
        "        \"For 'Price_Suggestion', extract 'category', 'brand', and 'price' as an integer if available. \"\n",
        "        \"For 'Moderation', return no entities. \"\n",
        "        \"Respond only with a single JSON object. \"\n",
        "        \"Example for Price_Suggestion: {'intent': 'Price_Suggestion', 'entities': {'category': 'mobile', 'price': 30000}} \"\n",
        "        \"Example for Moderation: {'intent': 'Moderation'} \"\n",
        "        \"Example for General_Question: {'intent': 'General_Question'}\"\n",
        "    )\n",
        "\n",
        "    intent_messages = [\n",
        "        {\"role\": \"system\", \"content\": intent_system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "    intent_response_str = call_ollama(intent_messages)\n",
        "\n",
        "    try:\n",
        "        if isinstance(intent_response_str, str):\n",
        "            intent_response = json.loads(intent_response_str)\n",
        "        else:\n",
        "            intent_response = intent_response_str\n",
        "\n",
        "        intent = intent_response.get('intent', 'General_Question')\n",
        "        entities = intent_response.get('entities', {})\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error parsing intent JSON. Defaulting to General_Question.\")\n",
        "        intent = 'General_Question'\n",
        "        entities = {}\n",
        "\n",
        "    final_response = {}\n",
        "\n",
        "    if intent == 'Moderation':\n",
        "        moderation_result = chat_moderation_agent(user_message)\n",
        "        final_response = moderation_result\n",
        "        final_response['type'] = 'Moderation'\n",
        "        print(\"Intent: Moderation\")\n",
        "\n",
        "    elif intent == 'Price_Suggestion':\n",
        "        # Add conversational memory\n",
        "        if 'category' not in entities and session.get('last_category'):\n",
        "            entities['category'] = session['last_category']\n",
        "            print(\"Using category from session memory:\", session['last_category'])\n",
        "\n",
        "        # Save current category for next turn's memory\n",
        "        if 'category' in entities:\n",
        "            session['last_category'] = entities['category']\n",
        "\n",
        "        print(\"Intent: Price_Suggestion. Entities:\", entities)\n",
        "        retrieved_data = retrieve_data(entities)\n",
        "        context = format_context(retrieved_data)\n",
        "\n",
        "        suggestion_system_instruction = \"You are a helpful assistant providing product recommendations and price suggestions based on the provided context. If no relevant products are found, state that and provide a suggestion for a better price or category to search for. Respond in a friendly, conversational tone and in JSON format with a single field 'response_text'.\"\n",
        "        suggestion_messages = [\n",
        "            {\"role\": \"system\", \"content\": suggestion_system_instruction},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nUser Query: {user_message}\"}\n",
        "        ]\n",
        "\n",
        "        llm_response_content = call_ollama(suggestion_messages, format=\"json\")\n",
        "        final_response = {\n",
        "            \"prompt\": suggestion_messages,\n",
        "            \"response\": llm_response_content,\n",
        "            \"type\": \"Price_Suggestion\"\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        print(\"Intent: General_Question or parsing failed.\")\n",
        "        final_response = {\n",
        "            \"prompt\": [],\n",
        "            \"response\": \"I'm sorry, I can only provide product suggestions and moderate chats. Can you please rephrase your query?\",\n",
        "            \"type\": \"General_Question\"\n",
        "        }\n",
        "\n",
        "    arc_cache.put(user_message, final_response)\n",
        "    return final_response\n",
        "\n",
        "\n",
        "def chat_moderation_agent(chat_message):\n",
        "    user_query = f\"Categorize the following chat message and provide a reason. Message: '{chat_message}'\"\n",
        "\n",
        "    system_instruction = (\n",
        "        \"You are a chat moderation bot. Your task is to classify a given chat message into one of the following categories: 'Safe', 'Abusive', 'Spam', or 'Contains Mobile Number'. \"\n",
        "        \"Your response must be in JSON format with 'category' and 'reason' fields. \"\n",
        "        \"Rules: \"\n",
        "        \"- 'Abusive': Contains offensive, hateful, or threatening language. \"\n",
        "        \"- 'Spam': Repetitive, irrelevant, or promotional content. \"\n",
        "        \"- 'Contains Mobile Number': Contains a phone number or other explicit contact information. This includes formats with country codes (e.g., +375646), and standard 10-digit numbers (e.g., 9876543210). \"\n",
        "        \"- 'Safe': All other messages.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_query}\n",
        "    ]\n",
        "\n",
        "    llm_response_content = call_ollama(messages, format=\"json\")\n",
        "\n",
        "    return {\n",
        "        \"prompt\": messages,\n",
        "        \"response\": llm_response_content\n",
        "    }\n",
        "\n",
        "# --- Flask App and ngrok Interface ---\n",
        "app = Flask(__name__)\n",
        "app.secret_key = 'super_secret_key_for_session' # Required for session management\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Unified Chat Agent</title>\n",
        "        <style>\n",
        "            body { font-family: sans-serif; margin: 2rem; background-color: #f4f4f9; }\n",
        "            .container { max-width: 700px; margin: auto; padding: 1.5rem; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); background-color: white; }\n",
        "            h1 { color: #333; }\n",
        "            #chat-container { height: 400px; overflow-y: scroll; border: 1px solid #ccc; padding: 1rem; border-radius: 4px; margin-bottom: 1rem; }\n",
        "            .user-message { text-align: right; background-color: #dcf8c6; padding: 0.75rem; border-radius: 10px; margin-bottom: 0.5rem; word-wrap: break-word; }\n",
        "            .bot-message { text-align: left; background-color: #e2e2e2; padding: 0.75rem; border-radius: 10px; margin-bottom: 0.5rem; word-wrap: break-word; }\n",
        "            #chat-form { display: flex; gap: 0.5rem; }\n",
        "            #message-input { flex: 1; padding: 0.75rem; border: 1px solid #ccc; border-radius: 4px; }\n",
        "            button { background-color: #007bff; color: white; padding: 0.75rem 1.5rem; border: none; border-radius: 4px; cursor: pointer; }\n",
        "            button:hover { background-color: #0056b3; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Unified Chat Agent</h1>\n",
        "        <p>Ask for a price suggestion (e.g., \"What's a good mobile under 30k?\") or test moderation (e.g., \"I am selling a phone for 9876543210\").</p>\n",
        "\n",
        "        <div class=\"container\">\n",
        "            <div id=\"chat-container\"></div>\n",
        "            <form id=\"chat-form\">\n",
        "                <input type=\"text\" id=\"message-input\" placeholder=\"Type your message...\" required>\n",
        "                <button type=\"submit\">Send</button>\n",
        "            </form>\n",
        "        </div>\n",
        "\n",
        "        <script>\n",
        "            const chatForm = document.getElementById('chat-form');\n",
        "            const messageInput = document.getElementById('message-input');\n",
        "            const chatContainer = document.getElementById('chat-container');\n",
        "\n",
        "            function addMessage(sender, message) {\n",
        "                const messageDiv = document.createElement('div');\n",
        "                messageDiv.className = sender === 'user' ? 'user-message' : 'bot-message';\n",
        "                messageDiv.innerHTML = message.replace(/\\\\n/g, '<br>');\n",
        "                chatContainer.appendChild(messageDiv);\n",
        "                chatContainer.scrollTop = chatContainer.scrollHeight;\n",
        "            }\n",
        "\n",
        "            chatForm.addEventListener('submit', async (e) => {\n",
        "                e.preventDefault();\n",
        "                const userMessage = messageInput.value;\n",
        "                addMessage('user', userMessage);\n",
        "                messageInput.value = '';\n",
        "\n",
        "                addMessage('bot', 'Typing...');\n",
        "\n",
        "                try {\n",
        "                    const response = await fetch('/chat', {\n",
        "                        method: 'POST',\n",
        "                        headers: { 'Content-Type': 'application/json' },\n",
        "                        body: JSON.stringify({ message: userMessage }),\n",
        "                    });\n",
        "\n",
        "                    if (!response.ok) {\n",
        "                        throw new Error(`HTTP error! status: ${response.status}`);\n",
        "                    }\n",
        "                    const data = await response.json();\n",
        "\n",
        "                    chatContainer.lastChild.textContent = '';\n",
        "\n",
        "                    if (data.status === 'success') {\n",
        "                        if (data.result && data.result.type === 'Moderation') {\n",
        "                            const llm_result = JSON.parse(data.result.response);\n",
        "                            addMessage('bot', `<b>Moderation Result:</b><br><b>Category:</b> ${llm_result.category}<br><b>Reason:</b> ${llm_result.reason}`);\n",
        "                        } else if (data.result && data.result.type === 'Price_Suggestion') {\n",
        "                            const llm_result = JSON.parse(data.result.response);\n",
        "                            addMessage('bot', llm_result.response_text);\n",
        "                        } else {\n",
        "                             addMessage('bot', data.result.response);\n",
        "                        }\n",
        "                    } else {\n",
        "                        addMessage('bot', 'An error occurred. Please try again.');\n",
        "                    }\n",
        "                } catch (error) {\n",
        "                    chatContainer.lastChild.textContent = '';\n",
        "                    addMessage('bot', 'An error occurred. Please try again later.');\n",
        "                }\n",
        "            });\n",
        "        </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_content\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat_api():\n",
        "    if not request.is_json:\n",
        "        return jsonify({\"status\": \"error\", \"reason\": \"Request body must be JSON\"}), 400\n",
        "\n",
        "    user_message = request.json.get('message')\n",
        "    if not user_message:\n",
        "        return jsonify({\"status\": \"error\", \"reason\": \"Missing 'message' field in JSON body\"}), 400\n",
        "\n",
        "    result = router_agent(user_message)\n",
        "    return jsonify({\n",
        "        \"status\": \"success\",\n",
        "        \"result\": result\n",
        "    })\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == '__main__':\n",
        "    if OLLAMA_MODEL:\n",
        "        try:\n",
        "            ngrok_secret = userdata.get('ngrok')\n",
        "            if not ngrok_secret:\n",
        "                raise ValueError(\"ngrok secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "            ngrok.set_auth_token(ngrok_secret)\n",
        "\n",
        "            public_url = ngrok.connect(5000)\n",
        "            print(f\"\\n* Flask app is running and accessible at: {public_url}\")\n",
        "            print(\"\\n* Open this URL in your browser to use the unified chat interface.\")\n",
        "\n",
        "            app.run(port=5000)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error starting Flask or ngrok: {e}\")\n",
        "    else:\n",
        "        print(\"\\nSkipping web server setup due to Ollama model not loaded.\")# Install necessary libraries\n",
        "!pip install Flask pyngrok --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from flask import Flask, request, jsonify, session\n",
        "\n",
        "# --- Ollama Setup ---\n",
        "def start_ollama_server(model_name=\"mistral\"):\n",
        "    try:\n",
        "        print(\"Starting Ollama setup...\")\n",
        "        subprocess.run(\"curl -fsSL https://ollama.com/install.sh | sh\", shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "        ollama_thread = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"], check=True))\n",
        "        ollama_thread.start()\n",
        "        time.sleep(5)\n",
        "        print(f\"Pulling model: {model_name}...\")\n",
        "        subprocess.run([\"ollama\", \"pull\", model_name], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "        print(f\"Model {model_name} pulled successfully.\")\n",
        "        return model_name\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Ollama setup: {e}\")\n",
        "        return None\n",
        "\n",
        "# Start the Ollama server and pull the model\n",
        "OLLAMA_MODEL = start_ollama_server()\n",
        "\n",
        "\n",
        "# --- Data Ingestion and Local Knowledge Base ---\n",
        "file_path = '/content/drive/MyDrive/db.txt'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if not df.empty:\n",
        "    category_index = {key: list(value) for key, value in df.groupby('category').groups.items()}\n",
        "    brand_index = {key: list(value) for key, value in df.groupby('brand').groups.items()}\n",
        "    condition_index = {key: list(value) for key, value in df.groupby('condition').groups.items()}\n",
        "\n",
        "# Mapping for common user terms to database categories\n",
        "CATEGORY_MAPPING = {\n",
        "    'phone': 'Mobile',\n",
        "    'phones': 'Mobile',\n",
        "    'mobile': 'Mobile',\n",
        "    'mobiles': 'Mobile',\n",
        "    'camera': 'Camera',\n",
        "    'lenses': 'Camera Lens',\n",
        "    'lens': 'Camera Lens',\n",
        "    'tablet': 'Tablet',\n",
        "    'ipads': 'Tablet',\n",
        "    'tv': 'TV',\n",
        "    'televisions': 'TV',\n",
        "}\n",
        "\n",
        "# --- Adaptive Replacement Cache (ARC) Implementation ---\n",
        "class ARCCache:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.t1 = OrderedDict()\n",
        "        self.t2 = OrderedDict()\n",
        "        self.b1 = OrderedDict()\n",
        "        self.b2 = OrderedDict()\n",
        "        self.p = 0\n",
        "\n",
        "    def get(self, key):\n",
        "        if key in self.t1:\n",
        "            value = self.t1.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "            return value\n",
        "        elif key in self.t2:\n",
        "            value = self.t2.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "            return value\n",
        "        return None\n",
        "\n",
        "    def put(self, key, value):\n",
        "        if key in self.t1:\n",
        "            self.t1[key] = value\n",
        "            return\n",
        "        if key in self.t2:\n",
        "            self.t2[key] = value\n",
        "            return\n",
        "\n",
        "        if len(self.t1) + len(self.t2) == self.capacity:\n",
        "            if len(self.t1) > self.p:\n",
        "                old_key, _ = self.t1.popitem(last=False)\n",
        "                self.b1[old_key] = None\n",
        "            else:\n",
        "                old_key, _ = self.t2.popitem(last=False)\n",
        "                self.b2[old_key] = None\n",
        "\n",
        "        if key in self.b1:\n",
        "            self.p = min(self.capacity, self.p + max(1, len(self.b2) / (len(self.b1) + 1)))\n",
        "            self.b1.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "        elif key in self.b2:\n",
        "            self.p = max(0, self.p - max(1, len(self.b1) / (len(self.b2) + 1)))\n",
        "            self.b2.pop(key)\n",
        "            self.t2[key] = value\n",
        "            self.t2.move_to_end(key)\n",
        "        else:\n",
        "            self.t1[key] = value\n",
        "            self.t1.move_to_end(key)\n",
        "\n",
        "        if len(self.b1) > self.capacity - self.p:\n",
        "            self.b1.popitem(last=False)\n",
        "        if len(self.b2) > self.p:\n",
        "            self.b2.popitem(last=False)\n",
        "\n",
        "cache_capacity = 100\n",
        "arc_cache = ARCCache(cache_capacity)\n",
        "\n",
        "\n",
        "# --- Core LLM Communication Function ---\n",
        "def call_ollama(messages, format=\"json\"):\n",
        "    if OLLAMA_MODEL is None:\n",
        "        return {\"status\": \"error\", \"reason\": \"Ollama server not running or model not available.\"}\n",
        "\n",
        "    try:\n",
        "        url = \"http://localhost:11434/api/chat\"\n",
        "        payload = {\n",
        "            \"model\": OLLAMA_MODEL,\n",
        "            \"messages\": messages,\n",
        "            \"stream\": False,\n",
        "            \"format\": format\n",
        "        }\n",
        "        response = requests.post(url, json=payload, timeout=300)\n",
        "        response.raise_for_status()\n",
        "        response_json = response.json()\n",
        "        return response_json['message']['content']\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"Error calling Ollama API: {e}. Attempting to parse raw response.\")\n",
        "        return json.dumps({\"status\": \"error\", \"reason\": f\"Ollama API returned an error: {e}\"})\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error calling Ollama API: {e}\")\n",
        "        return json.dumps({\"status\": \"error\", \"reason\": f\"Error connecting to Ollama API: {e}\"})\n",
        "\n",
        "\n",
        "# --- Retrieval Function with Price Filter ---\n",
        "def retrieve_data(query_info):\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "    filtered_df = df.copy()\n",
        "\n",
        "    # Use mapped category if it exists\n",
        "    category = query_info.get('category')\n",
        "    if category:\n",
        "        mapped_category = CATEGORY_MAPPING.get(category.lower(), category)\n",
        "        filtered_df = filtered_df[filtered_df['category'].str.contains(mapped_category, case=False, na=False)]\n",
        "\n",
        "    if 'brand' in query_info and query_info['brand']:\n",
        "        filtered_df = filtered_df[filtered_df['brand'].str.contains(query_info['brand'], case=False, na=False)]\n",
        "    if 'condition' in query_info and query_info['condition']:\n",
        "        filtered_df = filtered_df[filtered_df['condition'].str.contains(query_info['condition'], case=False, na=False)]\n",
        "\n",
        "    if 'price' in query_info and query_info['price']:\n",
        "        try:\n",
        "            price_limit = int(query_info['price'])\n",
        "            filtered_df = filtered_df[pd.to_numeric(filtered_df['asking_price'], errors='coerce') <= price_limit]\n",
        "        except (ValueError, KeyError):\n",
        "            pass\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def format_context(retrieved_data):\n",
        "    if retrieved_data.empty:\n",
        "        return \"No relevant products found in the database.\"\n",
        "    context = \"Relevant products found in the database:\\n\"\n",
        "    for _, row in retrieved_data.iterrows():\n",
        "        context += f\"- ID: {row['id']}, Title: {row['title']}, Category: {row['category']}, Brand: {row['brand']}, Condition: {row['condition']}, Age: {row['age_months']} months, Asking Price: {row['asking_price']}, Location: {row['location']}\\n\"\n",
        "    return context\n",
        "\n",
        "\n",
        "# --- Unified Chat Router Agent ---\n",
        "def router_agent(user_message):\n",
        "    cached_result = arc_cache.get(user_message)\n",
        "    if cached_result:\n",
        "        print(\"Cache hit!\")\n",
        "        return cached_result\n",
        "\n",
        "    greetings = [\"hi\", \"hello\", \"hey\", \"hallo\"]\n",
        "    if user_message.strip().lower() in greetings:\n",
        "        return {\n",
        "            \"prompt\": [],\n",
        "            \"response\": \"Hello! I can help you with product price suggestions or moderate chat messages. What can I do for you?\",\n",
        "            \"type\": \"Greeting\"\n",
        "        }\n",
        "\n",
        "    intent_system_instruction = (\n",
        "        \"You are an intent classifier and entity extractor. Analyze the user's message to determine the primary intent and extract key entities. \"\n",
        "        \"Intents are: 'Price_Suggestion', 'Moderation', or 'General_Question'. \"\n",
        "        \"For 'Price_Suggestion', extract 'category', 'brand', and 'price' as an integer if available. \"\n",
        "        \"For 'Moderation', return no entities. \"\n",
        "        \"Respond only with a single JSON object. \"\n",
        "        \"Example for Price_Suggestion: {'intent': 'Price_Suggestion', 'entities': {'category': 'mobile', 'price': 30000}} \"\n",
        "        \"Example for Moderation: {'intent': 'Moderation'} \"\n",
        "        \"Example for General_Question: {'intent': 'General_Question'}\"\n",
        "    )\n",
        "\n",
        "    intent_messages = [\n",
        "        {\"role\": \"system\", \"content\": intent_system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "    intent_response_str = call_ollama(intent_messages)\n",
        "\n",
        "    try:\n",
        "        if isinstance(intent_response_str, str):\n",
        "            intent_response = json.loads(intent_response_str)\n",
        "        else:\n",
        "            intent_response = intent_response_str\n",
        "\n",
        "        intent = intent_response.get('intent', 'General_Question')\n",
        "        entities = intent_response.get('entities', {})\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error parsing intent JSON. Defaulting to General_Question.\")\n",
        "        intent = 'General_Question'\n",
        "        entities = {}\n",
        "\n",
        "    final_response = {}\n",
        "\n",
        "    if intent == 'Moderation':\n",
        "        moderation_result = chat_moderation_agent(user_message)\n",
        "        final_response = moderation_result\n",
        "        final_response['type'] = 'Moderation'\n",
        "        print(\"Intent: Moderation\")\n",
        "\n",
        "    elif intent == 'Price_Suggestion':\n",
        "        # Add conversational memory\n",
        "        if 'category' not in entities and session.get('last_category'):\n",
        "            entities['category'] = session['last_category']\n",
        "            print(\"Using category from session memory:\", session['last_category'])\n",
        "\n",
        "        # Save current category for next turn's memory\n",
        "        if 'category' in entities:\n",
        "            session['last_category'] = entities['category']\n",
        "\n",
        "        print(\"Intent: Price_Suggestion. Entities:\", entities)\n",
        "        retrieved_data = retrieve_data(entities)\n",
        "        context = format_context(retrieved_data)\n",
        "\n",
        "        suggestion_system_instruction = \"You are a helpful assistant providing product recommendations and price suggestions based on the provided context. If no relevant products are found, state that and provide a suggestion for a better price or category to search for. Respond in a friendly, conversational tone and in JSON format with a single field 'response_text'.\"\n",
        "        suggestion_messages = [\n",
        "            {\"role\": \"system\", \"content\": suggestion_system_instruction},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nUser Query: {user_message}\"}\n",
        "        ]\n",
        "\n",
        "        llm_response_content = call_ollama(suggestion_messages, format=\"json\")\n",
        "        final_response = {\n",
        "            \"prompt\": suggestion_messages,\n",
        "            \"response\": llm_response_content,\n",
        "            \"type\": \"Price_Suggestion\"\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        print(\"Intent: General_Question or parsing failed.\")\n",
        "        final_response = {\n",
        "            \"prompt\": [],\n",
        "            \"response\": \"I'm sorry, I can only provide product suggestions and moderate chats. Can you please rephrase your query?\",\n",
        "            \"type\": \"General_Question\"\n",
        "        }\n",
        "\n",
        "    arc_cache.put(user_message, final_response)\n",
        "    return final_response\n",
        "\n",
        "\n",
        "def chat_moderation_agent(chat_message):\n",
        "    user_query = f\"Categorize the following chat message and provide a reason. Message: '{chat_message}'\"\n",
        "\n",
        "    system_instruction = (\n",
        "        \"You are a chat moderation bot. Your task is to classify a given chat message into one of the following categories: 'Safe', 'Abusive', 'Spam', or 'Contains Mobile Number'. \"\n",
        "        \"Your response must be in JSON format with 'category' and 'reason' fields. \"\n",
        "        \"Rules: \"\n",
        "        \"- 'Abusive': Contains offensive, hateful, or threatening language. \"\n",
        "        \"- 'Spam': Repetitive, irrelevant, or promotional content. \"\n",
        "        \"- 'Contains Mobile Number': Contains a phone number or other explicit contact information. This includes formats with country codes (e.g., +375646), and standard 10-digit numbers (e.g., 9876543210). \"\n",
        "        \"- 'Safe': All other messages.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_query}\n",
        "    ]\n",
        "\n",
        "    llm_response_content = call_ollama(messages, format=\"json\")\n",
        "\n",
        "    return {\n",
        "        \"prompt\": messages,\n",
        "        \"response\": llm_response_content\n",
        "    }\n",
        "\n",
        "# --- Flask App and ngrok Interface ---\n",
        "app = Flask(__name__)\n",
        "app.secret_key = 'super_secret_key_for_session' # Required for session management\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Unified Chat Agent</title>\n",
        "        <style>\n",
        "            body { font-family: sans-serif; margin: 2rem; background-color: #f4f4f9; }\n",
        "            .container { max-width: 700px; margin: auto; padding: 1.5rem; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); background-color: white; }\n",
        "            h1 { color: #333; }\n",
        "            #chat-container { height: 400px; overflow-y: scroll; border: 1px solid #ccc; padding: 1rem; border-radius: 4px; margin-bottom: 1rem; }\n",
        "            .user-message { text-align: right; background-color: #dcf8c6; padding: 0.75rem; border-radius: 10px; margin-bottom: 0.5rem; word-wrap: break-word; }\n",
        "            .bot-message { text-align: left; background-color: #e2e2e2; padding: 0.75rem; border-radius: 10px; margin-bottom: 0.5rem; word-wrap: break-word; }\n",
        "            #chat-form { display: flex; gap: 0.5rem; }\n",
        "            #message-input { flex: 1; padding: 0.75rem; border: 1px solid #ccc; border-radius: 4px; }\n",
        "            button { background-color: #007bff; color: white; padding: 0.75rem 1.5rem; border: none; border-radius: 4px; cursor: pointer; }\n",
        "            button:hover { background-color: #0056b3; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Unified Chat Agent</h1>\n",
        "        <p>Ask for a price suggestion (e.g., \"What's a good mobile under 30k?\") or test moderation (e.g., \"I am selling a phone for 9876543210\").</p>\n",
        "\n",
        "        <div class=\"container\">\n",
        "            <div id=\"chat-container\"></div>\n",
        "            <form id=\"chat-form\">\n",
        "                <input type=\"text\" id=\"message-input\" placeholder=\"Type your message...\" required>\n",
        "                <button type=\"submit\">Send</button>\n",
        "            </form>\n",
        "        </div>\n",
        "\n",
        "        <script>\n",
        "            const chatForm = document.getElementById('chat-form');\n",
        "            const messageInput = document.getElementById('message-input');\n",
        "            const chatContainer = document.getElementById('chat-container');\n",
        "\n",
        "            function addMessage(sender, message) {\n",
        "                const messageDiv = document.createElement('div');\n",
        "                messageDiv.className = sender === 'user' ? 'user-message' : 'bot-message';\n",
        "                messageDiv.innerHTML = message.replace(/\\\\n/g, '<br>');\n",
        "                chatContainer.appendChild(messageDiv);\n",
        "                chatContainer.scrollTop = chatContainer.scrollHeight;\n",
        "            }\n",
        "\n",
        "            chatForm.addEventListener('submit', async (e) => {\n",
        "                e.preventDefault();\n",
        "                const userMessage = messageInput.value;\n",
        "                addMessage('user', userMessage);\n",
        "                messageInput.value = '';\n",
        "\n",
        "                addMessage('bot', 'Typing...');\n",
        "\n",
        "                try {\n",
        "                    const response = await fetch('/chat', {\n",
        "                        method: 'POST',\n",
        "                        headers: { 'Content-Type': 'application/json' },\n",
        "                        body: JSON.stringify({ message: userMessage }),\n",
        "                    });\n",
        "\n",
        "                    if (!response.ok) {\n",
        "                        throw new Error(`HTTP error! status: ${response.status}`);\n",
        "                    }\n",
        "                    const data = await response.json();\n",
        "\n",
        "                    chatContainer.lastChild.textContent = '';\n",
        "\n",
        "                    if (data.status === 'success') {\n",
        "                        if (data.result && data.result.type === 'Moderation') {\n",
        "                            const llm_result = JSON.parse(data.result.response);\n",
        "                            addMessage('bot', `<b>Moderation Result:</b><br><b>Category:</b> ${llm_result.category}<br><b>Reason:</b> ${llm_result.reason}`);\n",
        "                        } else if (data.result && data.result.type === 'Price_Suggestion') {\n",
        "                            const llm_result = JSON.parse(data.result.response);\n",
        "                            addMessage('bot', llm_result.response_text);\n",
        "                        } else {\n",
        "                             addMessage('bot', data.result.response);\n",
        "                        }\n",
        "                    } else {\n",
        "                        addMessage('bot', 'An error occurred. Please try again.');\n",
        "                    }\n",
        "                } catch (error) {\n",
        "                    chatContainer.lastChild.textContent = '';\n",
        "                    addMessage('bot', 'An error occurred. Please try again later.');\n",
        "                }\n",
        "            });\n",
        "        </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_content\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat_api():\n",
        "    if not request.is_json:\n",
        "        return jsonify({\"status\": \"error\", \"reason\": \"Request body must be JSON\"}), 400\n",
        "\n",
        "    user_message = request.json.get('message')\n",
        "    if not user_message:\n",
        "        return jsonify({\"status\": \"error\", \"reason\": \"Missing 'message' field in JSON body\"}), 400\n",
        "\n",
        "    result = router_agent(user_message)\n",
        "    return jsonify({\n",
        "        \"status\": \"success\",\n",
        "        \"result\": result\n",
        "    })\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == '__main__':\n",
        "    if OLLAMA_MODEL:\n",
        "        try:\n",
        "            ngrok_secret = userdata.get('ngrok')\n",
        "            if not ngrok_secret:\n",
        "                raise ValueError(\"ngrok secret not found. Please add it to Colab secrets.\")\n",
        "\n",
        "            ngrok.set_auth_token(ngrok_secret)\n",
        "\n",
        "            public_url = ngrok.connect(5000)\n",
        "            print(f\"\\n* Flask app is running and accessible at: {public_url}\")\n",
        "            print(\"\\n* Open this URL in your browser to use the unified chat interface.\")\n",
        "\n",
        "            app.run(port=5000)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error starting Flask or ngrok: {e}\")\n",
        "    else:\n",
        "        print(\"\\nSkipping web server setup due to Ollama model not loaded.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1YGb-0-dqGyOEMmeNzkjVVSf7IWgzI_Or",
      "authorship_tag": "ABX9TyOjM150poq8emvp9VflXFEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}